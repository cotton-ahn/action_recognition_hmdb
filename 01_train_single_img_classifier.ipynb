{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision.transforms._transforms_video import ToTensorVideo, RandomResizedCropVideo, RandomHorizontalFlipVideo, NormalizeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "vid_path = '../../dataset/hmdb51_vid/'\n",
    "split_path = '../../dataset/hmdb51_split/'\n",
    "class_names = sorted(os.listdir(vid_path))\n",
    "class_embed = pickle.load(open('./metas/class_embed_sbert.pkl', 'rb'))\n",
    "print(class_embed[class_names[0]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embed = list()\n",
    "for n_i, name in enumerate(class_names):\n",
    "    total_embed.append(torch.FloatTensor(class_embed[name]).view(1, -1))\n",
    "total_embed = torch.cat(total_embed, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "feat_dim = 1024\n",
    "inter_dim = feat_dim//4\n",
    "lang_dim = 768\n",
    "clip_len = 20\n",
    "fps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7431c8a1218d4862a1753faa6bd58ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=423.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46536a269fae4ad1be77e181693e9e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=423.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "to_tensor = ToTensorVideo()\n",
    "random_crop = RandomResizedCropVideo(img_size, scale=(0.95, 1.05))\n",
    "random_flip = RandomHorizontalFlipVideo()\n",
    "normalizer = NormalizeVideo(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "curr_transform = torchvision.transforms.Compose([to_tensor, random_crop, random_flip, normalizer])\n",
    "\n",
    "train_dataset = torchvision.datasets.HMDB51(vid_path, \n",
    "                                           split_path, \n",
    "                                           frames_per_clip=clip_len, step_between_clips=clip_len//2, \n",
    "                                           frame_rate=fps, fold=1, train=True, \n",
    "                                           transform=curr_transform)\n",
    "test_dataset = torchvision.datasets.HMDB51(vid_path, \n",
    "                                           split_path, \n",
    "                                           frames_per_clip=clip_len, step_between_clips=clip_len//2, \n",
    "                                           frame_rate=fps, fold=1, train=False, \n",
    "                                           transform=curr_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = 1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, feat_dim, inter_dim, lang_dim, img_size, clip_len):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.feat_dim = feat_dim\n",
    "        self.inter_dim = inter_dim\n",
    "        self.lang_dim = lang_dim\n",
    "        self.img_size = img_size\n",
    "        self.clip_len = clip_len\n",
    "    \n",
    "        self.backbone = torchvision.models.resnet50(pretrained=True, progress=True)\n",
    "        for c_idx, child in enumerate(self.backbone.children()):\n",
    "            if c_idx < 5:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        modules=list(self.backbone.children())[:7]\n",
    "        self.backbone=nn.Sequential(*modules)\n",
    "        \n",
    "        self.adain_linear_1 = nn.Linear(lang_dim, feat_dim*2)\n",
    "        self.adain_linear_2 = nn.Linear(lang_dim, inter_dim*2)\n",
    "        self.cls_query_linear = nn.Linear(lang_dim, feat_dim)\n",
    "        \n",
    "        self.dec_conv1 = nn.Conv2d(feat_dim, inter_dim, 3, padding=1, bias=False)\n",
    "        self.dec_relu = nn.ReLU()\n",
    "        self.dec_dropout = nn.Dropout(0.1)\n",
    "        self.dec_conv2 = nn.Conv2d(inter_dim, 1, 1)\n",
    "        \n",
    "        self.attn_feat_linear = nn.Linear(feat_dim, feat_dim)\n",
    "        \n",
    "        self.cos = nn.CosineSimilarity(dim=1)\n",
    "        \n",
    "    def forward(self, img, cls_embed, total_embed):\n",
    "        total_adain_1 = self.adain_linear_1(total_embed)\n",
    "        total_adain_2 = self.adain_linear_2(total_embed)\n",
    "\n",
    "        cls_embed = cls_embed.view(1, -1)\n",
    "        cls_query = self.cls_query_linear(cls_embed)\n",
    "        \n",
    "        # get feature\n",
    "        feature = self.backbone(img)\n",
    "        \n",
    "        # calculate mean and std of feature\n",
    "        feature_mean = torch.mean(feature, dim=[2, 3], keepdim=True)# B D\n",
    "        feature_std = torch.std(feature, dim=[2, 3], keepdim=True) # B D\n",
    "        \n",
    "        # adain to the feature\n",
    "        normed_feature = torch.div(feature - feature_mean, feature_std+1e-8)\n",
    "        adain_feature = torch.mul(normed_feature, total_adain_1[:, :self.feat_dim, None, None])\n",
    "        adain_feature = torch.add(adain_feature, total_adain_1[:, self.feat_dim:, None, None])\n",
    "        \n",
    "        # decoder to adain_ed result\n",
    "        dec_result = self.dec_conv1(adain_feature)\n",
    "        dec_result = self.dec_dropout(self.dec_relu(dec_result))\n",
    "        \n",
    "        # adain to the decoder result\n",
    "        dec_mean = torch.mean(dec_result, dim=[2, 3], keepdim=True)\n",
    "        dec_std = torch.std(dec_result, dim=[2, 3], keepdim=True)\n",
    "        \n",
    "        dec_result = torch.div(dec_result - dec_mean, dec_std+1e-8)\n",
    "        dec_result = torch.mul(dec_result, total_adain_2[:, :self.inter_dim, None, None])\n",
    "        dec_result = torch.add(dec_result, total_adain_2[:, self.inter_dim:, None, None])\n",
    "        \n",
    "        # final layer of decoder\n",
    "        attn_map = self.dec_conv2(dec_result)\n",
    "        \n",
    "        # get attended feature\n",
    "        feature_flat = feature.squeeze(0).view(self.feat_dim, -1)\n",
    "        attn_map_flat = attn_map.squeeze(1).view(attn_map.shape[0], -1)\n",
    "        \n",
    "        attn_feat = torch.mm(attn_map_flat, feature_flat.permute(1, 0))\n",
    "        \n",
    "        # calculate the cosine similarity between attn_feat (N_c x D) and current cls_query (1 x D)\n",
    "        cos_sim = self.cos(self.attn_feat_linear(attn_feat), cls_query)\n",
    "        \n",
    "        # return the logit, attention map(dec_result). Later, I would only need attn_feat. \n",
    "        return cos_sim, attn_map\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(feat_dim, inter_dim, lang_dim, img_size, clip_len)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, sample, total_embed):\n",
    "    model.train()\n",
    "    \n",
    "    # define loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # reset gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # calculate input\n",
    "    frame_index = np.random.randint(low=0, high=clip_len)\n",
    "    img = sample[0][0][:, frame_index:(frame_index+1), :, :].permute(1, 0, 2, 3)\n",
    "    cls_embed = torch.FloatTensor(class_embed[class_names[sample[2].item()]])\n",
    "    cls_label = sample[2]   \n",
    "\n",
    "    # get result\n",
    "    cos_sim, attn_map = model(img.to(device), \n",
    "                              cls_embed.to(device), \n",
    "                              total_embed.to(device))\n",
    "    \n",
    "    # optimize one step\n",
    "    loss = criterion(cos_sim.view(1, -1), cls_label.to(device))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, sample, total_embed):\n",
    "    model.eval()\n",
    "    \n",
    "    # define loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # calculate input\n",
    "        frame_index = clip_len//2\n",
    "        img = sample[0][0][:, frame_index:(frame_index+1), :, :].permute(1, 0, 2, 3)\n",
    "        cls_embed = torch.FloatTensor(class_embed[class_names[sample[2].item()]])\n",
    "        cls_label = sample[2]   \n",
    "\n",
    "        # get result\n",
    "        cos_sim, attn_map = model(img.to(device), \n",
    "                                  cls_embed.to(device),\n",
    "                                  total_embed.to(device))\n",
    "        \n",
    "        # get loss and accuracy\n",
    "        loss = criterion(cos_sim.view(1, -1), cls_label.to(device))\n",
    "        esti_label = torch.argmax(cos_sim)\n",
    "        accu = esti_label.item() == cls_label.item() \n",
    "    \n",
    "    return loss.item(), accu, attn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb749a37390645c99baa5253fbc79b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1436.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_epoch = 200\n",
    "dir_name = 'first'\n",
    "writer = SummaryWriter(logdir=os.path.join('./runs', dir_name))\n",
    "n_iter = 0\n",
    "max_accu = 0.0\n",
    "save_stride = 50\n",
    "tmp_path = './checkpoint.tar'\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    if epoch > 0:\n",
    "        checkpoint = torch.load(tmp_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    with tqdm(total=len(train_dataloader)) as pbar:\n",
    "        for idx, sample in enumerate(train_dataloader):\n",
    "            curr_loss = train(model, optimizer, sample, total_embed)\n",
    "            writer.add_scalar('train/loss', curr_loss, n_iter)\n",
    "            n_iter += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': curr_loss,\n",
    "            }, tmp_path)\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_accu = 0.0\n",
    "    np.random.seed(epoch)\n",
    "    plot_idx = np.random.randint(0, len(test_dataloader))\n",
    "    \n",
    "    checkpoint = torch.load(tmp_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    with tqdm(total=len(test_dataloader)) as pbar:\n",
    "        for idx, sample in enumerate(test_dataloader):\n",
    "            loss, accu, attn_map = test(model, sample, total_embed)\n",
    "            test_loss += loss / len(test_dataloader)\n",
    "            if accu:\n",
    "                test_accu += accu / len(test_dataloader)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if idx == plot_idx:\n",
    "                fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "                ax[0].imshow(sample[0][0][:, 10, :, :].permute(1, 2, 0))\n",
    "                idx = sample[2].item()\n",
    "                ax[1].imshow(attn_map[idx][0].cpu())\n",
    "                plt.suptitle(accu)\n",
    "                plt.savefig('./tmp_img.png')\n",
    "                tmp_img = torch.FloatTensor(plt.imread('./tmp_img.png')[:, :, :3]).permute(2, 0, 1)\n",
    "                writer.add_image('test/attn_map', tmp_img, epoch)           \n",
    "                \n",
    "    writer.add_scalar('test/loss', test_loss, epoch)\n",
    "    writer.add_scalar('test/accuracy', test_accu, epoch)\n",
    "    \n",
    "    max_accu = max(test_accu, max_accu)\n",
    "    \n",
    "    if test_accu == max_accu:\n",
    "        torch.save(model.state_dict(), './single_model/{}_best.pth'.format(dir_name))\n",
    "    torch.save(model.state_dict(), './single_model/{}_recent.pth'.format(dir_name))\n",
    "    if (epoch+1) % save_stride == 0:\n",
    "        torch.save(model.state_dict(), './single_model/{}_{}.pth'.format(dir_name, epoch+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imread('./tmp_img.png')[:, :, :3].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
